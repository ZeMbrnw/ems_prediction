# EMS_Respone_Prediction
NYC EMS Response Time Prediction and Disparity Analysis

# Projectâ€™s Function
The core objective of this project is to analyze the performance and equity of NYC's emergency medical response system. Addressing the rising trend in EMS response times over the years,a predictive model (XGBoost Regressor) was developed to forecast incident duration. This is used to identify geographical bottlenecks and service disparities across the city,aiming to provide data-driven recommendations for resource allocation and systemic improvement.

# Dataset and Provenence 
The EMS dataset originates from New York City Emergency Medical Services records, accessed via the NYC Open Data portal. It was generated by EMS personnel during dispatch and patient care, recording timestamps, response times, call types, and other categorical indicators.The data is trustworthy and authentic, as it comes directly from an official government source open to the public. For this model data from the year 2024 was trained. 

# Pipeline / Architecture
The architecture uses Apache Airflow to manage a linear ETL/ML pipeline, where Amazon S3 serves as the data lake and as a secure checkpoint between stages. The pipeline uses Pandas for data manipulation, Scikit-learn for model preparation, and XGBoost for prediction. The final results are exported to a CSV file consumed by Power BI and Matplotlib.

# Data Quality Assessment
The initial data required significant cleaning due to missing values and extreme outliers. The data quality was assessed by filtering out null incident timestamps, imputing median values for numerical features (response times, severity), and clipping extreme outliers (using 1st and 99th percentiles) to prevent model bias.

# Data Transformation Models Used
- Transformation: Feature engineering extracted temporal data (hour, DOW, month). Categorical features (Borough, call type) were label encoded.
- Model: An XGBoost Regressor was trained to predict incident response time (in seconds), achieving a Mean Absolute Error (MAE) of 427.07 seconds on the test set. This result is effective for strategic resource allocation, as the 7-minute error margin is precise enough to reliably identify severe geographic and temporal bottlenecks within the system.
- Instructions: This pipeline requires the xgboost and scikit-learn libraries. Ensure you install all required libraries using the provided requirements.txt file in your environment.


### Infographic Data Pipeline

 ![Data Pipeline](https://i.imgur.com/2Kg1fNv.png)

### Infographic Visualization Results

 ![PowerBI01](https://i.imgur.com/TVpSI4p.png)
 

 ![Combined Error](https://i.imgur.com/eLesd8Q.png)

### Infographic Matplotlib Feature Importance and Prediction Errors 

 ![Matplotlib Feature ](https://i.imgur.com/wRbB8y4.png)

 
 ![PowerBI02](https://i.imgur.com/Uksf4fW.png)

 
# Conclusions & Recommendations
The pipeline is stable but relies on sequential execution within a single Airflow worker, which seems to be a bottleneck for true scalability. For production,it is recommended migrating the Airflow execution environment to a more robust environment. This transition allows the pipeline to manage much larger datasets and process data volumes more efficiently by distributing the computationally intensive XGBoost training across multiple, simultaneous processes. This ensures the system remains fast and responsive even as data volume grows.

The project's innovation lies in its direct focus on equity and predictive capability. It moves beyond standard reporting (average response time) to deliver a clear feature importance ranking (showing variables, like 'borough' or 'hour', drive delay) and a map-ready predictive error metric for visualizing service disparity.

The main limitation encountered was the fragility of the execution environment (Docker/AWS) related to process memory allocation (Static TLS block error) when importing heavy libraries such as KERAS LSTM model. While solved by switching to XGBoost and using local script imports, it highlights a resource constraint that limits the use of memory-intensive deep learning models in the future.

The next step is to implement the Demand Forecasting component. This requires a separate data aggregation pipeline branch to count calls per hour/borough and train a new time-series model. This transition from only predicting delay to also predicting demand would empower the system to proactively adjust resource allocation before demand surges occur.
